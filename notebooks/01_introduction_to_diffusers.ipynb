{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Diffusers üß®\n",
    "\n",
    "Welcome to the first unit of the Diffusion Models course! In this notebook, we'll explore the basics of the ü§ó Diffusers library and learn how to generate images using state-of-the-art diffusion models.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what diffusion models are and how they work\n",
    "- Know how to use the Diffusers library to generate images\n",
    "- Be familiar with different schedulers and their effects\n",
    "- Understand key parameters like guidance scale and inference steps\n",
    "\n",
    "## What are Diffusion Models?\n",
    "\n",
    "Diffusion models are a class of generative models that learn to generate data by reversing a gradual noising process. They work by:\n",
    "\n",
    "1. **Forward Process**: Gradually adding noise to training images until they become pure noise\n",
    "2. **Reverse Process**: Learning to remove noise step by step to generate new images\n",
    "\n",
    "The most popular diffusion models for text-to-image generation include:\n",
    "- Stable Diffusion\n",
    "- DALL-E 2\n",
    "- Imagen\n",
    "- Midjourney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Login to HuggingFace (if token is available)\n",
    "hf_token = os.getenv(\"HUGGING_FACE_WRITE_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Logged in to HuggingFace\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No HuggingFace token found. Some models may not be accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your First Diffusion Model\n",
    "\n",
    "Let's start by loading a Stable Diffusion model. We'll use the `runwayml/stable-diffusion-v1-5` model, which is one of the most popular and accessible models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stable Diffusion pipeline\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "    safety_checker=None,  # Disable for educational purposes\n",
    "    requires_safety_checker=False\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable memory efficient attention\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Image Generation\n",
    "\n",
    "Now let's generate our first image! We'll start with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our prompt\n",
    "prompt = \"a photograph of an astronaut riding a horse\"\n",
    "\n",
    "print(f\"Generating image with prompt: '{prompt}'\")\n",
    "\n",
    "# Generate the image\n",
    "with torch.autocast(device):\n",
    "    image = pipe(prompt).images[0]\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Generated Image: {prompt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save the image\n",
    "image.save(\"astronaut_horse.png\")\n",
    "print(\"Image saved as 'astronaut_horse.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Key Parameters\n",
    "\n",
    "Diffusion models have several important parameters that control the generation process. Let's explore the most important ones:\n",
    "\n",
    "### 1. Number of Inference Steps\n",
    "\n",
    "This controls how many denoising steps the model takes. More steps generally mean higher quality but slower generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different numbers of inference steps\n",
    "prompt = \"a beautiful landscape with mountains and a lake\"\n",
    "steps_list = [10, 25, 50]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, steps in enumerate(steps_list):\n",
    "    print(f\"Generating with {steps} steps...\")\n",
    "    \n",
    "    with torch.autocast(device):\n",
    "        image = pipe(\n",
    "            prompt, \n",
    "            num_inference_steps=steps,\n",
    "            generator=torch.Generator(device=device).manual_seed(42)  # For reproducibility\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"{steps} steps\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Guidance Scale\n",
    "\n",
    "This parameter controls how closely the model follows your prompt. Higher values make the model follow the prompt more strictly, but can reduce creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "prompt = \"a cute robot painting a masterpiece\"\n",
    "guidance_scales = [1.0, 7.5, 15.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "    print(f\"Generating with guidance scale {scale}...\")\n",
    "    \n",
    "    with torch.autocast(device):\n",
    "        image = pipe(\n",
    "            prompt, \n",
    "            guidance_scale=scale,\n",
    "            num_inference_steps=25,\n",
    "            generator=torch.Generator(device=device).manual_seed(42)\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"Guidance Scale: {scale}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Negative Prompts\n",
    "\n",
    "Negative prompts tell the model what you DON'T want in your image. This is a powerful technique for improving image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without negative prompts\n",
    "prompt = \"a portrait of a person\"\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly, bad anatomy\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Without negative prompt\n",
    "print(\"Generating without negative prompt...\")\n",
    "with torch.autocast(device):\n",
    "    image1 = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=25,\n",
    "        generator=torch.Generator(device=device).manual_seed(42)\n",
    "    ).images[0]\n",
    "\n",
    "# With negative prompt\n",
    "print(\"Generating with negative prompt...\")\n",
    "with torch.autocast(device):\n",
    "    image2 = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=25,\n",
    "        generator=torch.Generator(device=device).manual_seed(42)\n",
    "    ).images[0]\n",
    "\n",
    "axes[0].imshow(image1)\n",
    "axes[0].set_title(\"Without Negative Prompt\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image2)\n",
    "axes[1].set_title(\"With Negative Prompt\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Schedulers\n",
    "\n",
    "Schedulers control how noise is removed during the generation process. Different schedulers can produce different results and have different speed/quality tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different schedulers\n",
    "from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler, EulerDiscreteScheduler\n",
    "\n",
    "prompt = \"a magical forest with glowing mushrooms\"\n",
    "\n",
    "# Define schedulers to compare\n",
    "schedulers = {\n",
    "    \"DDIM\": DDIMScheduler.from_config(pipe.scheduler.config),\n",
    "    \"PNDM\": PNDMScheduler.from_config(pipe.scheduler.config),\n",
    "    \"LMS\": LMSDiscreteScheduler.from_config(pipe.scheduler.config),\n",
    "    \"Euler\": EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, scheduler) in enumerate(schedulers.items()):\n",
    "    print(f\"Generating with {name} scheduler...\")\n",
    "    \n",
    "    # Set the scheduler\n",
    "    pipe.scheduler = scheduler\n",
    "    \n",
    "    with torch.autocast(device):\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=25,\n",
    "            generator=torch.Generator(device=device).manual_seed(42)\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"{name} Scheduler\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Different Art Styles\n",
    "\n",
    "One of the most exciting aspects of text-to-image models is their ability to generate images in different artistic styles. Let's explore this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different art styles\n",
    "base_prompt = \"a cat sitting on a windowsill\"\n",
    "styles = [\n",
    "    \"oil painting\",\n",
    "    \"watercolor\",\n",
    "    \"digital art\",\n",
    "    \"pencil sketch\",\n",
    "    \"anime style\",\n",
    "    \"photorealistic\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, style in enumerate(styles):\n",
    "    styled_prompt = f\"{base_prompt}, {style}\"\n",
    "    print(f\"Generating: {styled_prompt}\")\n",
    "    \n",
    "    with torch.autocast(device):\n",
    "        image = pipe(\n",
    "            styled_prompt,\n",
    "            num_inference_steps=25,\n",
    "            generator=torch.Generator(device=device).manual_seed(42 + i)\n",
    "        ).images[0]\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(style.title())\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation\n",
    "\n",
    "You can generate multiple images at once for the same prompt to explore variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple variations of the same prompt\n",
    "prompt = \"a cozy coffee shop in autumn\"\n",
    "num_images = 4\n",
    "\n",
    "print(f\"Generating {num_images} variations of: '{prompt}'\")\n",
    "\n",
    "with torch.autocast(device):\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        num_images_per_prompt=num_images,\n",
    "        num_inference_steps=25\n",
    "    ).images\n",
    "\n",
    "# Display all variations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"Variation {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create Your Own Generations\n",
    "\n",
    "Now it's your turn! Try creating images with your own prompts. Here are some ideas to get you started:\n",
    "\n",
    "- \"a futuristic city at sunset\"\n",
    "- \"a dragon made of flowers\"\n",
    "- \"a steampunk airship flying through clouds\"\n",
    "- \"a minimalist bedroom with plants\"\n",
    "- \"a vintage car in a cyberpunk setting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try your own prompts here\n",
    "your_prompt = \"\"  # Add your prompt here\n",
    "\n",
    "if your_prompt:\n",
    "    print(f\"Generating your image: '{your_prompt}'\")\n",
    "    \n",
    "    with torch.autocast(device):\n",
    "        your_image = pipe(\n",
    "            your_prompt,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5\n",
    "        ).images[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(your_image)\n",
    "    plt.title(f\"Your Creation: {your_prompt}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save your creation\n",
    "    your_image.save(\"my_creation.png\")\n",
    "    print(\"Your image saved as 'my_creation.png'\")\n",
    "else:\n",
    "    print(\"Add your prompt to the 'your_prompt' variable above and run this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the introduction to diffusers. Here's what you've learned:\n",
    "\n",
    "‚úÖ **What diffusion models are** and how they work  \n",
    "‚úÖ **How to load and use** the Diffusers library  \n",
    "‚úÖ **Key parameters** like inference steps, guidance scale, and negative prompts  \n",
    "‚úÖ **Different schedulers** and their effects  \n",
    "‚úÖ **Art style control** through prompting  \n",
    "‚úÖ **Batch generation** for exploring variations  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebooks, we'll explore:\n",
    "- Image-to-image generation\n",
    "- Inpainting and outpainting\n",
    "- Fine-tuning your own models\n",
    "- Advanced prompting techniques\n",
    "\n",
    "Keep experimenting and have fun creating! üé®‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To free up GPU memory, you can delete the pipeline when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "del pipe\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"‚úÖ Memory cleaned up!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}